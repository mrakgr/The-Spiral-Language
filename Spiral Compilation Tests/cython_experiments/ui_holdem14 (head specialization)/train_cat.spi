// The functions here propagate rewards as categorical probability vectors.

inl vs_self game (combine, to_cat, empty : _ * (_ -> tensor) * _) (batch_size, p) =
    let rec loop (l : a u64 _) =
        inl terminal_indices : ra u64 _ = am.empty
        inl terminal_rewards : ra u64 r2 = am.empty
        inl actions_indices : ra u64 _ = am.empty
        inl data : ra u64 _ = am.empty
        inl nexts : ra u64 _ = am.empty
        l |> am.iteri fun i => function
            | Action: player_state,game_state,pid,actions,next => 
                rm.add actions_indices i
                rm.add data (player_state,game_state,pid,actions)
                rm.add nexts next
            | Terminal: _,_,x =>
                rm.add terminal_indices i
                rm.add terminal_rewards x
        inl actions_rewards =
            if 0 < length data then
                inl (cs : a _ _),(update : tensor -> tensor) = p data
                am.generic.map2 (<|) nexts cs |> loop |> update
            else empty
        combine (actions_indices, actions_rewards, terminal_indices, to_cat terminal_rewards)
    loop (am.init batch_size fun _ => game pl2_init)

inl vs_one game (combine, to_cat, empty : _ * (_ -> tensor) * _) (batch_size, p1, p2) =
    let rec loop (l : a u64 _) =
        inl terminal_indices : ra u64 _ = am.empty
        inl terminal_rewards : ra u64 r2 = am.empty
        inl p1_actions_indices : ra u64 _ = am.empty
        inl p2_actions_indices : ra u64 _ = am.empty
        inl p1_data : ra u64 _ = am.empty
        inl p2_data : ra u64 _ = am.empty
        inl p1_nexts : ra u64 _ = am.empty
        inl p2_nexts : ra u64 _ = am.empty
        inl pids : ra u64 _ = am.empty
        l |> am.iteri fun i => function
            | Action: player_state,game_state,pid,actions,next =>
                if pid = 0 then
                    rm.add p1_actions_indices i
                    rm.add p1_data (player_state,game_state,pid,actions)
                    rm.add p1_nexts next
                else
                    rm.add p2_actions_indices i
                    rm.add p2_data (player_state,game_state,pid,actions)
                    rm.add p2_nexts next
                rm.add pids pid
            | Terminal: _,_,x =>
                rm.add terminal_indices i
                rm.add terminal_rewards x
        inl p1_actions_rewards, p2_actions_rewards =
            if 0 < length pids then
                inl p1_cs,p1_update : a _ _ * (tensor -> tensor) = p1 p1_data
                inl p2_cs,p2_update : a _ _ * (tensor -> tensor) = p2 p2_data
                inl p1_results = am.generic.map2 (<|) p1_nexts p1_cs
                inl p2_results = am.generic.map2 (<|) p2_nexts p2_cs
                inl rs = loop (am.append p1_results p2_results)
                inl len = length p1_cs
                inl slice_from forall t. (x : t) (i : u64) : t = $"!x[!i:]"
                inl slice_near_to forall t. (x : t) (i : u64) : t = $"!x[:!i]"
                p1_update (slice_near_to rs len), p2_update (slice_from rs len)
            else empty, empty
        combine (p1_actions_indices, p1_actions_rewards, p2_actions_indices, p2_actions_rewards, terminal_indices, to_cat terminal_rewards)
    loop (am.init batch_size fun _ => game pl2_init)
