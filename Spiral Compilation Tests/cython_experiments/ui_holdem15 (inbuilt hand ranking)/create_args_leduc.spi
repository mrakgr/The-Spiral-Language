open leduc
type agent_dict = agent.tabular.agent_dict (list (choice2 card action)) (list agent.neural_leduc.value)
inl main () =
    // `exp`ected and `cat`egorical
    inl vs_self = namedtuple "VsSelf" {exp=train.vs_self leduc.game; cat=train_cat.vs_self leduc.game}
    inl vs_one = namedtuple "VsOne" {exp=train.vs_one leduc.game; cat=train_cat.vs_one leduc.game}
    inl selector (leduc_actions {is_fold is_raise} : actions) : a st action =
        inl x : ra st _ = am.empty
        if is_fold then rm.add x Fold
        rm.add x Call
        if is_raise then rm.add x Raise
        $"numpy.array(!x,copy=False)"
    inl neural = train.neural agent.neural_leduc.extractor agent.neural_leduc.schema() (3,selector)
    inl uniform_player : ra u64 (pl2 card action * leduc_state * pid * actions) -> a u64 (log_prob * action) * (obj -> obj) =
        agent.uniform.policy selector
    inl tabular =
        inl create_policy : agent_dict * bool * bool * f32 -> ra u64 (pl2 card action * leduc_state * u8 * actions) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2) =
            agent.tabular.policy agent.neural_leduc.extractor selector
        inl create_agent () : agent_dict = dictm.empty
        inl optimize : agent_dict -> () = agent.tabular.optimize
        inl average : agent_dict * agent_dict -> () = agent.tabular.average
        inl head_multiply_ : agent_dict * _ -> _ = agent.tabular.head_multiply_
        namedtuple "Tabular" {create_policy create_agent head_multiply_ optimize average}

    record {vs_self vs_one neural uniform_player tabular}